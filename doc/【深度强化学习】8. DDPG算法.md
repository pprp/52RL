# 【深度强化学习】8. DDPG算法

【DataWhale打卡】DDPG算法 Deep Deterministric Policy Gradient

视频参考自：https://www.bilibili.com/video/BV1yv411i7xd?p=19



## 1、思维导图

![](https://img-blog.csdnimg.cn/20201107105717989.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

## 2. 详解

DDPG是解决连续性控制问题的一个算法，但是和PPO不同，PPO输出是一个策略，是一个概率分布。而DDPG输出的是一个动作。

DDPG是采用的也是Actor-Critic架构，是基于DQN进行改进的。DQN中的action space必须是离散的，所以不能处理连续的动作空间的问题。DDPG在其基础上进行改动，引入了一个Actor Network,让一个网络来的输出来得到连续的动作空间。

| 对比   | AC               | DDPG       |
| ------ | ---------------- | ---------- |
| Actor  | 输出的是概率分布 | 输出是动作 |
| Critic | 预估V值          | 预估Q值    |
| 更新   | 带权重梯度更新   | 梯度上升   |

![](https://img-blog.csdnimg.cn/20201107121658876.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

优化Q网络的时候，如果Q-target也在不停的变动，那就会造成更新困难。类似DQN，DDPG也采取了固定网络结构的方法，先冻结target网络，更新参数以后，再把参数赋值到target网络。所以需要的是四个网络：

- actor
- critic
- target actor
- target critic

通过上图可以看出，DDPG(也是一种Actor-Critic方法)，其实也是一种时序差分的方法，结合了基于Value-based和Policy-Based方法。其中Policy是Actor，用于给出动作；价值函数是Critic，评价Actor给出的Action的好坏，产生时序差分信号用于指导价值函数和策略函数的更新。

## 3. 代码

















