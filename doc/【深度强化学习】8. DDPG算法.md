# 【深度强化学习】8. DDPG算法

【DataWhale打卡】DDPG算法 Deep Deterministric Policy Gradient

视频参考自：https://www.bilibili.com/video/BV1yv411i7xd?p=19



## 1、思维导图

![](https://img-blog.csdnimg.cn/20201107105717989.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

## 2. 详解

DDPG是解决连续性控制问题的一个算法，但是和PPO不同，PPO输出是一个策略，是一个概率分布。而DDPG输出的是一个动作。

DDPG是采用的也是Actor-Critic架构，是基于DQN进行改进的。DQN中的action space必须是离散的，所以不能处理连续的动作空间的问题。DDPG在其基础上进行改动，引入了一个Actor Network,让一个网络来的输出来得到连续的动作空间。

| 对比   | AC               | DDPG       |
| ------ | ---------------- | ---------- |
| Actor  | 输出的是概率分布 | 输出是动作 |
| Critic | 预估V值          | 预估Q值    |
| 更新   | 带权重梯度更新   | 梯度上升   |





