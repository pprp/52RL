# 【深度强化学习】4. Policy Gradient

【Datawhale打卡】十一的时候自己看过一遍，李宏毅老师讲的很好，对数学小白也很友好，但是由于没有做笔记（敲代码），看完以后脑袋里空落落的。趁着这次打卡活动，重新看一遍，果然好多细节需要重头梳理一遍。

## 1. 新概念/符号

- **policy（策略）：** 每一个actor中会有对应的策略，这个策略决定了actor的行为。(给定一个state，policy会决定action)。**policy记为 $\pi$ 。**
- **Return（回报）：** 一个回合（Episode）所得到的所有的reward的总和，也称为Total reward。**一般地，用 $R$ 来表示。**
- **Trajectory（轨迹 $\tau$ ）：** 一个试验中将environment 输出的 $s$ 跟 actor 输出的行为 $a$，把这个 $s$ 跟 $a$ 全部串起来形成的集合，称为Trajectory，即 $\text { Trajectory } \tau=\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}\right\}$。
- **Reward function：** 根据在某一个 state 采取的某一个 action 决定说现在这个行为可以得到多少的分数，它是一个 function。也就是给一个 $s_1$，$a_1$，它告诉你得到 $r_1$。给它 $s_2$ ，$a_2$，它告诉你得到 $r_2$。 把所有的 $r$ 都加起来，就得到了 $R(\tau)$ ，代表某一个 trajectory $\tau$ 的 reward。
- **Expected reward：** $\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)=E_{\tau \sim p_{\theta}(\tau)}[R(\tau)]$。

| 符号     | 解释                                                         |
| -------- | ------------------------------------------------------------ |
| $\tau$   | 轨迹，游戏从开始到结束的s、a串（$\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}\right\}$） |
| episode  | 一个游戏回合，从开始到结束                                   |
| $\pi$    | Policy 策略的代指符号                                        |
| $\theta$ | Policy $\pi$中的参数                                         |



## 2. 三个组成部分

![示意图](https://img-blog.csdnimg.cn/20201029122722788.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

强化学习通常有以下组成部分，actor, environment, reward。具体过程如上图所示，构成了一个完整的轨迹Trajectory: 
$$
\text { Trajectory } \tau=\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}\right\}
$$
每一个 trajectory，你可以计算它发生的概率。假设现在 actor 的参数已经被给定了话，就是 $\theta$。根据 $\theta$，你其实可以计算某一个 trajectory 发生的概率，你可以计算某一个回合，某一个 episode 里面， 发生这样子状况的概率。
$$
\begin{aligned}
p_{\theta}(\tau)
&=p\left(s_{1}\right) p_{\theta}\left(a_{1} | s_{1}\right) p\left(s_{2} | s_{1}, a_{1}\right) p_{\theta}\left(a_{2} | s_{2}\right) p\left(s_{3} | s_{2}, a_{2}\right) \cdots \\
&=p\left(s_{1}\right) \prod_{t=1}^{T} p_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)
\end{aligned}
$$

> 注：以上的P函数分为两种！
>
> 1 $p\left(s_{t+1} | s_{t}, a_{t}\right)$函数代表环境，当前环境，如果在状态st下使用at，转化到$s_{t+1}$的概率。
>
> 2 $p_{\theta}(a_t|s_t)$函数代表actor/agent的policy，如果在状态st下使用at的概率。

**Reward Function:** 根据在某一个 state 采取的某一个 action 决定说现在这个行为可以得到多少的分数。 

给它 $s_1$，$a_1$，它告诉你得到 $r_1$。给它 $s_2$ ，$a_2$，它告诉你得到 $r_2$。 把所有的 $r$ 都加起来，就得到了 $R(\tau)$ , 目标就是通过调整actor的参数$\theta$, 让R的值越大越好。由于actor采取action的时候存在随机性，所以R也是一个随机变量，采用以下公式估计其期望值：
$$
\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)=E_{\tau \sim p_{\theta}(\tau)}[R(\tau)]
$$
这个计算方式也就是：穷举所有可能的 trajectory $\tau$， 每一个 trajectory $\tau$ 都有一个概率。同时也可以表达为期望的形式，从 $p_{\theta}(\tau)$ 这个 distribution sample 一个 trajectory $\tau$，然后计算 $R(\tau)$ 的期望值，就是你的 expected reward。 目标就是 maximize expected reward。

## 3. Gradient Ascent

对以下式子希望得到maximize expected reward, 要使用Gradient Ascent算法，先计算R的梯度gradient。
$$
\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)
$$
由于只有$p_{\theta}(\tau)$是和参数$\theta$相关的，所以梯度只需要对这部分求即可。

存在以下公式：

由：
$$
\frac{d(lnx)}{dx}=\frac{1}{x}
$$
得：
$$
\nabla f(x)=f(x)\nabla logf(x)
$$
所以带入$p_\theta$以后有：
$$
\frac{\nabla p_\theta(\tau)}{p_\theta(\tau)}=\nabla log p_\theta(\tau)
$$
推导过程如下：
$$
\begin{aligned}
\nabla \bar{R}_{\theta}&=\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)\\&=\sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\&=
\sum_{\tau} R(\tau) p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) \\
&=E_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right]
\end{aligned}
$$
最终是一个期望，这个期望无法直接计算，只能通过sample一些轨迹$\tau$, 求解器平均值来计算梯度，有了梯度以后就可以更新参数，具体公式如下：
$$
\begin{aligned}
E_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right] &\approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right) \\
&=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
\end{aligned}
$$

> 注:  $p_{\theta}(\tau)$ 里面有两项，$p(s_{t+1}|s_t,a_t)$ 来自于 environment，$p_\theta(a_t|s_t)$ 是来自于 agent。
>
>  $p(s_{t+1}|s_t,a_t)$ 由环境决定从而与 $\theta$ 无关，因此 $\nabla \log p(s_{t+1}|s_t,a_t) =0$。因此 $\nabla p_{\theta}(\tau)=
> \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)$。

然后对推导得到的最终结果进行定性解释：

- 在sample到的轨迹中，某一个状态$s_t$, 要执行动作$a_t$。

- 如果$R(\tau)$是正的，那就要增加$(s_t,a_t)$的概率，让actor能够在下一次遇到$s_t$以后能以更高的概率选中$a_t$。
- 为负同理。

## 4. 实现/实做

具体实现过程如下：
$$
\theta \leftarrow \theta+\eta \nabla \bar{R_\theta} \\
\nabla \bar{R_\theta}=\frac{1}{N}\sum^N_{n=1}\sum^{T_n}_{t=1}R(\tau^n)\nabla log p_\theta(a_t^n|s_t^n)
$$
还需要收集一系列s和a的pair，以及对应的reward。将sample得到的s和a组成的pair带入到上面的gradient的式子中，然后计算其log probablitiy，然后取gradient，然后就可以进行更新了。

这一点和分类问题中的交叉熵有一点类似，可以按照以下方法进行理解：
$$
\text{标签值}\times log(\text{标签对应的概率})
$$
这样就和以上做到了形式上的一致（不是很严谨）。RL和一般分类问题不同的地方是loss前面乘上了weight R。

在这里会需要乘以一个奖励回报 $R$。这个奖励回报相当于是对这个真实 action 的评价，$R$ 具体越大，未来总收益越大，说明当前输出的这个真实的 action 就越好，这个 loss 就越需要重视。如果 $R$ 越小，那就说明做这个 action $a_t$ 并没有那么的好，loss 的权重就要小一点，优化力度就小一点。

### 4.1 TIP1 Add a Baseline

$$
\theta \leftarrow \theta+\eta \nabla \bar{R_\theta} \\
\nabla \bar{R_\theta}=\frac{1}{N}\sum^N_{n=1}\sum^{T_n}_{t=1}(R(\tau^n)-b)\nabla log p_\theta(a_t^n|s_t^n) \\
b \approx E[R(\tau)]
$$

![](https://img-blog.csdnimg.cn/20201029224610895.png#pic_center)

如果某些样本没有sample到，那其他动作的概率都会提升，它本身概率会下降，这就存在问题了，可以通过添加一个baseline，让reward不总是正的值。

### 4.2 TIP2 Assign Suitable Credit

下面这个式子的话，

$$
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(R\left(\tau^{n}\right)-b\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
$$

原来会做的事情是，在某一个 state，假设你执行了某一个 action a，它得到的 reward ，它前面乘上的这一项 $R(\tau^n)-b$, 这个值就可以理解为，当前s下使用动作a以后的好坏程度。

$R(\tau^n)$ 代表整个episode执行完以后得到的结果，由于强化学习具有延迟奖励的特点，可以考虑**以下改进**：更关注于近期得到的奖励，长远奖励要被削弱。这个想法实现如下：

$$
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(\sum_{t'=t}^{T_n}\gamma^{t'-t}r_{t'}^n-b\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
$$

其中$\gamma$代表的是0-1之间的小数，用于削弱长远的奖励，这就是discount fastor:

- 一般会设个 0.9 或 0.99，

* $\gamma = 0$ : 只关心即时奖励； 
* $\gamma = 1$ : 未来奖励等同于即时奖励。

然后就可以顺利引入Advantage Function，这个函数是依赖于状态s和动作a的，如下所示：

$$
A^\theta(s_t,a_t)
$$

代表在$s_t$状态下执行动作$a_t$到底有多好，可以带来多大的奖励。这个”好“代表的是相对优势，因为会减掉baseline，这个函数通常可以是由一个network（critic）估计出来的。

## 5. MC & TD

策略梯度可以用蒙特卡洛算法(MC)或者时序差分算法(TD)求解。

### 5.1 MC-REINFORCE

蒙特卡洛算法是回合更新，常见算法是REINFORCE。

$$
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}G_t^n \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
$$

可以理解为完成一个episode以后，拿这个episode的数据去学习，然后做一次更新。

$G_t$ 是未来总收益，$G_t$ 代表是从这个 step 后面，能拿到的收益之和是多少。

在代码上的处理上是先拿到每个 step 的 reward，然后计算每个 step 的未来总收益 $G_t$ 是多少，然后拿每个 $G_t$ 代入公式，去优化每一个 action 的输出。

编写代码时会有这样一个函数，输入每个 step 拿到的 reward，把这些 reward 转成每一个 step 的未来总收益。因为未来总收益是这样计算的：
$$
\begin{aligned}
G_{t} &=\sum_{k=t+1}^{T} \gamma^{k-t-1} r_{k} \\
&=r_{t+1}+\gamma G_{t+1}
\end{aligned}
$$
![](https://img-blog.csdnimg.cn/20201029224742106.png#pic_center)

先产生一个 episode 的数据，比如 $(s_1,a_1,G_1),(s_2,a_2,G_2),\cdots,(s_T,a_T,G_T)$。然后针对每个 action 来计算梯度。 

在代码上计算时，要拿到神经网络的输出。神经网络会输出每个 action 对应的概率值，然后还可以拿到实际的 action，把它转成 one-hot 向量乘一下，可以拿到 $\ln \pi(A_t|S_t,\theta)$  。

![REINFORCE流程图](https://img-blog.csdnimg.cn/20201029224812214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

### 5.2 TD-Actor Critic

时序差分是单步更新，更新频率更高，这里用Q-function来近似表示未来的收益。

$$
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}Q^n(s_t^n,a^n_t) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
$$

时序差分强化学习能够在知道结果之前就开始学习，相比蒙特卡洛强化学习，其更快速、灵活。

## 6. 问答

Q: 在一个process中，一个具体的trajectory $s_1$,$a_1$, $s_2$ , $a_2$ 出现的概率取决于什么？

A：

1. 一部分是 **environment 的行为**， environment 的 function 它内部的参数或内部的规则长什么样子。 $p(s_{t+1}|s_t,a_t)$这一项代表的是 environment， environment 这一项通常你是无法控制它的，因为那个是人家写好的，或者已经客观存在的。
2. 另一部分是 **agent 的行为**，你能控制的是 $p_\theta(a_t|s_t)$。给定一个 $s_t$， actor 要采取什么样的 $a_t$ 会取决于你 actor 的参数 $\theta$， 所以这部分是 actor 可以自己控制的。随着 actor 的行为不同，每个同样的 trajectory， 它就会有不同的出现的概率。

---

Q: 可以使用哪些方法来进行gradient ascent的计算？

A：用 gradient ascent 来 update 参数，对于原来的参数 $\theta$ ，可以将原始的 $\theta$  加上更新的 gradient 这一项，再乘以一个 learning rate，learning rate 其实也是要调的，和神经网络一样，可以使用 Adam、RMSProp 等优化器对其进行调整。

---

Q: Advantage Function作用：

A: 在某一个 state $s_t$ 执行某一个 action $a_t$，相较于其他可能的 action，它有多好。它在意的不是一个绝对的好，而是相对的好，即相对优势(relative advantage)。因为会减掉一个 b，减掉一个 baseline， 所以这个东西是相对的好，不是绝对的好。 $A^{\theta}\left(s_{t}, a_{t}\right)$ 通常可以是由一个 network estimate 出来的，这个 network 叫做 critic。

---

Q:对于梯度策略的两种方法，蒙特卡洛（MC）强化学习和时序差分（TD）强化学习两个方法有什么联系和区别

A: **两者的更新频率不同**，蒙特卡洛强化学习方法是**每一个episode更新一次**，即需要经历完整的状态序列后再更新（比如贪吃蛇游戏，贪吃蛇“死了”游戏结束后再更新）

对于时序差分强化学习方法是**每一个step就更新一次** ，（比如贪吃蛇游戏，贪吃蛇每移动一次（或几次）就进行更新）。相对来说，时序差分强化学习方法比蒙特卡洛强化学习方法更新的频率更快。

时序差分强化学习能够在知道一个小step后就进行学习，相比于蒙特卡洛强化学习，其更加**快速、灵活**。

---



## 7. 参考

* [Intro to Reinforcement Learning (强化学习纲要）](https://github.com/zhoubolei/introRL)
* [神经网络与深度学习](https://nndl.github.io/)
* [百面深度学习](https://book.douban.com/subject/35043939/)