# 【深度强化学习】7. 稀疏奖励和模仿学习

【DataWhale打卡】李宏毅老师视频中的最后两部分，sparse reward和imitation learning。



[TOC]



##  1. Sparse Reward

Sparse Reward是强化学习中最重要的问题，也是最难解决的，仅仅通过exploration来进行探索是远远不够的。这一节将讲解如何解决Sparse Reward的问题。

### 1.1 Reward Shaping

可以刻意设计一些reward来引导达到最终的reward。

举例：小孩在学习（长远reward值很高）和游戏（短期reward值更高）的两者中做选择，很有可能就会选择游戏，这时候为了让小孩能够考虑长远，可以考虑让小孩在学习的时候设计人为的reward，期望能够引导到reward更高的情况。

相当于引入了domain knowledge，

![](https://img-blog.csdnimg.cn/2020110509484659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

在原有基础上加入了Intrinsic curiosity Module(ICM模块，给agent加入**好奇心**。输入是s1、a1、s2, 就会输出一个额外的reward，总奖励值中就会额外加入这个reward。

**ICM**

![](https://img-blog.csdnimg.cn/20201105095735975.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

Network1是额外学习出来的，根据at和st来预测下一个$\hat{s_{t+1}}$，然后将两者的插值作为reward的值，这样设置也就是说，如果下一个状态很难去预测，那么就给个更大的奖励，也就是鼓励冒险的意思。

但是有一个问题：以上reward的设置默认 **越难被预测状态就越好**。但是这个并不是总是成立的，**有的状态很难预测，但是并不代表它更重要**。

如何判断这个很难预测的状态是否是真的重要的呢？

![](https://img-blog.csdnimg.cn/20201105100832159.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

引入另外一个Network 2，此外还引入了一个特征提取器，Feature Ext来对状态进行特征提取。Network2接受来自$s_t和s_{t+}$的特征Vector作为输入，输出的值是一个action $\hat{a_t}$ (这个值的意义就是从st到$s_{t+1}$的话，要采取的action)，希望这个action和实际的action $a_t$越接近越好。

> Network1这部分计算让模型能够拥有足够的好奇心，下一个状态越难预测就给更高的reward。
>
> Network2这部分计算就是约束下一个状态，从st到$s_{t+1}$预测需要的action和实际采取的action越接近越好，防止提取得到的特征中存在的和action无关的信息。Network2的作用就是更好的优化Feature Ext这个特征提取器，从而让这个提取得到的特征，更加关注于对action影响较大的特征。

### 1.2 Curriculum Learning

为Agent的学习做一个规划，从简单到难，逐步学习。

在强化学习中，从简单的到困难就可以对应学习的序列的长度，短序列学习难度较低，长序列学习难度更高，所以可以先学习短序列，然后学习长序列。

再举一个例子，在躲避怪物的游戏中，从一开始就设置怪物速度到最大值，那很可能训练不起来的，可以让速度从低到高，循序渐进训练。

那么有没有**更通用的方法**来设计课程规划？

**Reverse Curriculum Generation**


- 给定一个目标状态$s_g$，也就是最终目标。
- 从$s_g$周围采样出一些状态点$s_i$,  比较接近于目标状态，加入到状态集合{S}。
- 从每个$s_i$就开始做互动，看是否能达到$s_g$， 每一个做互动的时候都会得到reward。
- 从状态集合{S}中删除那些reward过大或过小的状态$s_i$ 
- 在状态集合{S}周围继续采样比较接近于{S}中的$s_i$的状态，加入{S}集合。
- 重复以上操作

也就是说从目标开始，反向生成一系列课程，从难到易，循序渐进。

### 1.3 Hierarchical RL

多个Agent，High level的Agent负责制定目标，如同项目经理一般，剩下Low Level的Agent负责完成一个个小目标，如同普通码农一般。

- High Level的agent的目标是让Low Level的Agent完成它所设置的目标，如果没有完成，那就会有一个penalty惩罚。
- 如果agent达到错误的目标，那就假设最初的目标是错误的。

举两个例子：

![](https://img-blog.csdnimg.cn/20201105105511365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

第一个例子是蓝色的是agent，其目标是达到黄色的终点，那么这时候粉红色的点就代表high level的agent, 指导蓝色agent先接近红色agent，然后一步步达到黄色终点。

第二个例子是绕中间一点甩杆，目标是将杆子甩到黄色球的位置，这时候就需要粉红色的球做一个指引，从而达到最终的黄色位置。

## 2. Imitation Learning

比Sparse Reward更加极端，一点点reward都没有，应该如何是好呢？

- 很多环境中，无法得到reward的，也很难规定出reward，比如聊天机器人，无法判断怎样的聊天内容是好的，怎样是不好的，很难明确规定出来。

- Imitation Learning又叫做示范学习、学徒学习、观察学习。

- 在Imitation Learning中，有一些专家给出的demonstration，做了一个示范，那么agent就可以根据专家给出的demonstration来学习（这个过程中，无法显式地得到reward），从而不至于从零开始摸索。

- 在没有reward的情况下，手机专家的示意是可以做到的：
  - 比如：自动驾驶中，可以收集很多人类的开车记录，并进行研究和学习。
  - 比如：在聊天机器人中，可以收集一系列人与人的对话当作范例来学习，也是可行的。

- 有两种方法：Behavior Cloning和Inverse Reinforcement Learning(Inverse Optimal Contral)

### 2.1 Behavior Cloning

![](https://img-blog.csdnimg.cn/20201105112945499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

这就和监督学习一模一样了，在某个状态下，就采取某个行动，进行分类，然后监督学习。相当于是Expert做什么，Agent就要做一模一样的事情。

**存在问题1**

由于状态非常多，以自动驾驶为例，摄像机看到的图像画面是不可能穷举的，专家做出的演示不可能将现实世界中的全部状态都枚举完。所以Behavior Cloning的效果是很有限的，光是做behavior cloning是远远不够的，还需要dataset Aggregation。

**Dataset Aggregation**

期望训练的数据集是具有多样性的，尽可能覆盖多种极端情况。当遇到极端情况以后，再记录下Expert的操作，然后用这个新的数据来继续训练agent。

**存在问题2**

Agent会盲目学习Expert的所有行为，包括一些无关紧要的行为。由于Network的容量有限，盲目学习所有行为会导致容量不够。所以要确定什么是该学习的，什么是不该学习的。

**存在问题3**

在behavior cloning的过程中，其实训练集和测试集的分布是mismatch的。

RL中一个重要的特性是，前一个状态会对后一个状态产生影响。

这就需要Inverse Reinforcement Learning

### 2.2 Inverse Reinforcement Learning

传统的RL流程：

![](https://img-blog.csdnimg.cn/20201105115035518.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center) 

通过Reward Function，经过RL算法和env的交互，找到一个最优的Actor。

![](https://img-blog.csdnimg.cn/20201105114934796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center) 

Inverse RL中恰好相反，并没有reward function，只有一些列Expert的示例。

然后inverse RL通过反推得到Reward Function，好像很玄学。。

**具体做法**：

- Expert 玩游戏，记录所有的过程$\hat{\tau}$
- Actor也去玩游戏，也有所有游戏过程的记录$\tau$

- 然后推Reward function， 这里假设Expert得到的$\tau$永远是最好的。

  - 所以Expert的Reward分数要比Actor高。
    $$
    \sum_{n=1}^N R（\hat{\tau_n}）>\sum_{n=1}^N R(\tau_n)
    $$

  - 先射箭，再画靶

- 根据以上公式找到一个Reward function R

- 然后基于这个R进行训练学习得到一个Actor

- 然后循环,具体如下图所示

![](https://img-blog.csdnimg.cn/20201105124541225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

以上过程最让人困惑的过程就是Reward Runction R的选取了。实际上这里的Reward Function和传统的计算方式不同，可以采用的神经网络进行生成函数。只要是函数，就可以用神经网络进行拟合，所以这个过程也是一个可学习的过程。

> 这个过程和GAN又非常相似：
>
> Actor->Generator
>
> Reward Function->Discriminator
>
> 对比图：
>
> ![](https://img-blog.csdnimg.cn/20201105130026481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0REX1BQX0pK,size_16,color_FFFFFF,t_70#pic_center)

这方面还有很多可研究内容，如：Third Person Imitation Learning讲的是机器人的视角和人类不同的时候，如何将知识进行迁移。

## 3. 参考

https://www.bilibili.com/video/BV1MW411w79n?p=8

https://www.bilibili.com/video/BV1MW411w79n?p=7

https://github.com/datawhalechina/leedeeprl-notes



