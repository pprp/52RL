# 强化学习纲要-概要与基础

参考资料：https://github.com/zhoubolei/introRL

先导课程：线性代数、概率论、机器学习/数据挖掘/深度学习/模式识别

编程基础：Python, PyTorch

## 强化学习在做什么？

强化学习和监督学习有很大的区别：

1. 监督学习需要提供数据和对应的标签，训练数据和测试数据是独立同分布的，从而进行模式和特征的学习。

2. 强化学习不同，强化学习没有直接的标签进行指导，并且数据不是独立同分布的，前后数据有比较强的关系。强化学习可以在环境中进行探索和试错，根据实验的结果提取经验，从而学习到最佳策略。

**强化学习的目标**是训练一个agent，能够在不同的情况做出最佳的action，从而让系统给出的reward值最大化。

**流程如下**：agent会观察环境得到observation，然会采取一个action，环境受到这个action的作用，会反馈给agent一个reward，同时环境observation也发生了改变。循环往复，agent目标是为了从环境中获得最高reward奖励。

![]()

强化学习的特点：

- 输入的数据是序列化、前后有依赖的，并不是独立同分布的。
- 没有监督信息，每一步没有被告诉应该做什么。
- Trial-and-error exploration，exploration和exploitation之间的平衡：
  - exploration: 代表探索环境，尝试一些新的行为，这些行为有可能会带来巨大的收益，也可能减少收益。
  - exploitation: 就采取当前已知的可以获得最大收益的action。
- Reward Delay效应，当采取一个action以后，并不会立刻得到反馈。（重新组织一下）

## 强化学习应用案例

- alpha-go、alpha-zero围棋战胜李世石。
- 王者荣耀 绝悟AI 就是强化学习技术应用在MOBA游戏的一个典型例子。
- 可以将股票的买卖看作强化学习问题，如何操作能让收益极大化。
- Atari等电脑游戏。
- 机器人，比如如何让机械臂自己学会给一个杯子中倒水、抓取物体。
- DeepMind让Agent学习走路。
- 训练机械臂通过手指转魔方。
- 训练Agent穿衣服。

## 时序决策问题



Action采取可能会有深远的影响，得到的奖励Reward可能会有延迟（当前采取的某个action,需要等到很久以后，甚至游戏结束才能得到反馈）











## 知识点补充

Rollout：从游戏当前帧，生成很多局游戏，让当前的Model和环境交互，得到很多的观测（轨迹）,得到最终的最终reward，从而可以训练agent。







